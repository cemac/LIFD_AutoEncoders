{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKGKRqNgBjvk"
   },
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    <h1> Tutorial 5 </h1> \n",
    "    <h2> Auto Encoders </h2>\n",
    "</div>      \n",
    "\n",
    "\n",
    "# Overview\n",
    "\n",
    "This Jupyter notebook will walk you through building a simple AutoEcoder model and training it on the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) using [TensorFlow](https://www.tensorflow.org/). TensorFlow is a very powerful library written in Python and C++ that facilitates the building and training of neural networks.\n",
    "\n",
    "This tutorial is was adapted from Clare Bartholemew Presentation. Following Vikram Tiwaris's Kaggel [example] (https://www.kaggle.com/code/vikramtiwari/autoencoders-using-tf-keras-mnist/) based on [a keras blog](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "\n",
    "\n",
    "\n",
    "## The very basics\n",
    "\n",
    "If you know nothing about neural networks there is a [toy neural network python code example](https://github.com/cemac/LIFD_ENV_ML_NOTEBOOKS/tree/main/ToyNeuralNetwork) included in the [LIFD ENV ML Notebooks Repository]( https://github.com/cemac/LIFD_ENV_ML_NOTEBOOKS). Creating a 2 layer neural network to illustrate the fundamentals of how Neural Networks work and the equivalent code using the python machine learning library [keras](https://keras.io/). \n",
    "\n",
    "\n",
    "## Recommended reading \n",
    "\n",
    "\n",
    "* [All you need to know on Neural networks](https://towardsdatascience.com/nns-aynk-c34efe37f15a) \n",
    "* [Introduction to Neural Networks](https://victorzhou.com/blog/intro-to-neural-networks/)\n",
    "* [Auto Encoders](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798)\n",
    "\n",
    "\n",
    "## Additional Reading For Earth Science Applications\n",
    "\n",
    "At the end of this notebook, we'll take a brief look at some non-interactive earth science examples:\n",
    "\n",
    "* [Reconstructing missing SST data using AutoEncoders](https://gmd.copernicus.org/articles/13/1609/2020/) [(code hosted here)](https://github.com/gher-uliege/DINCAE)\n",
    "* [Using an AutoEncoder to mimic a GCM](https://brohan.org/ML_GCM/index.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "<h1> Machine Learning Theory </h1>\n",
    " \n",
    "## AutoEncoders \n",
    "    \n",
    "AutoEncoders are unsupervised learning technique that performs data encoding and decoding using feed forward neural networks made of two components:\n",
    "    \n",
    "* **Encoder** translates input data into lower dimensional space. (lower dimensional encoding is referred to as the latent space representation) \n",
    "* **Decoder** tries to reconstruct the original data from the lower dimensional data\n",
    "\n",
    "![AE fig](data_folder/figures/AEdiagram.png \"Auto Encoders\")\n",
    "    \n",
    "Auto-encoder is a complex mathematical model which trains on unlabeled as well as unclassified data and is used to map the input data to another compressed feature representation and from that feature representation reconstructing back the input data.\n",
    "\n",
    "**Characteristics of Auto Encoders:**\n",
    "    \n",
    "* Data specific: Non-generic - so will only work on data similar to what it has been trained on, making it an inpractical technique for compression problems\n",
    "* Lossy: There will be a degradation of data\n",
    "* Avenue for learning useful representations without the need for labels   \n",
    "    \n",
    "They can be used for a variety of different applications\n",
    "    \n",
    "1. [Dimensionality Reduction](). We have a whole notebook on this *coming soon*\n",
    "2. [Image denoising](#Denoising). Rather than searching for noise a representation of a noiseless image is extracted \n",
    "3. [Generation of image and time series data](https://brohan.org/ML_GCM/index.html) \n",
    "4. [Annomaly Detection](https://towardsdatascience.com/anomaly-detection-with-autoencoder-b4cdce4866a6)    \n",
    "    \n",
    "\n",
    "As you can see there are some obvious applications to Earth Science applications such as de-noising signals and anomaly detection, and there are some less obvious applications that we'll touch upon later such as using Auto Encoders to run a machine learning General Circulation Model\n",
    "\n",
    "</div>    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKGKRqNgBjvk"
   },
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "<h1> Python </h1>\n",
    "\n",
    "    \n",
    "    \n",
    "## Tensorflow \n",
    "    \n",
    "There are many machine learning python libraries available, [TensorFlow](https://www.tensorflow.org/) a is one such library. If you have GPUs on the machine you are using TensorFlow will automatically use them and run the code even faster!\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "* [Running Jupyter Notebooks](https://jupyter.readthedocs.io/en/latest/running.html#running)\n",
    "* [Tensorflow optimizers](https://www.tutorialspoint.com/tensorflow/tensorflow_optimizers.htm)\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffcc; padding: 10px;\">\n",
    "    \n",
    "<h1> Requirements </h1>\n",
    "\n",
    "These notebooks should run with the following requirements satisfied\n",
    "\n",
    "<h2> Python Packages: </h2>\n",
    "\n",
    "* Python 3\n",
    "* tensorflow > 2\n",
    "* numpy \n",
    "* matplotlib\n",
    "* scipy\n",
    "\n",
    "<h2> Data Requirements</h2>\n",
    "    \n",
    "This notebook referes to some data (no more than 1GB total) that will be downloaded via commands in this notebook.\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents:**\n",
    "\n",
    "1. [Download MNIST Dataset](#Download-MNIST-Dataset)\n",
    "2. [Building the Auto Encoder](#Building-the-Auto-Encoder)\n",
    "3. [Preparing to train](#Preparing-to-train)\n",
    "4. [Training](#Training)\n",
    "5. [Denoising](#Denoising)\n",
    "6. [Earth Science Examples](#Earth-Science-Examples)\n",
    "7. [Variational Autoencoders](#Variational-Autoencoders) \n",
    "8. [Visualizing latent space](#Visualizing-latent-space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "**Import python modules**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For readability: disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNoYx4yaCeGM"
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "# general file system utilites\n",
    "import os\n",
    "import requests\n",
    "# Plotting and standard numerical/stats tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "# Machine Learning Library Tensorflow and specific tools\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "Check if GPU's available. This code will run on standard CPU's too just a lots slower.\n",
    "    \n",
    "If no GPU's are detected, then pretrained model weights will be used set by the flag\n",
    "    \n",
    "```python\n",
    "train_local=False\n",
    "```\n",
    "\n",
    "**If you get a message saying no GPU available but you think there should be this means you have not managed to compile a GPU version of Tensorflow **    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aTyVAOT7b0LB",
    "outputId": "4ae92d05-f922-4f19-c242-b1c36e17271d"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if tf.config.list_physical_devices('GPU') else \"cpu\"\n",
    "if device=='cuda':\n",
    "  train_local=True\n",
    "  GPUs=True  \n",
    "  print('GPUs found Setting train_local to True')\n",
    "else:\n",
    "  train_local=False\n",
    "  GPUs=False\n",
    "  print(\"WARNING: No GPU available. Setting train_local to False\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "If you want to override this behaviour and train your own network if you don't mind waiting or if you have GPUS available but don't want to wait 20 mins training your own network later you can the the next cell to:\n",
    "\n",
    "```python\n",
    "override_train_local=True\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set as override_train_local=False to maintain default behaviour for your hardware\n",
    "override_train_local=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if override_train_local :\n",
    "    if GPUs:\n",
    "        train_local=False  \n",
    "        print('WARNING SETTING TRAIN LOCAL TO FALSE, PRETRAINED MODEL WILL BE USED')\n",
    "    else:\n",
    "        train_local=True\n",
    "        print('WARNING SETTING TRAIN LOCAL TO TRUE, WILL TRAIN MODEL OVER A LONG TIME ON CPUS')\n",
    "else: \n",
    "    print('Default behaviour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUQTC7zUaJK9"
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with np.load(path) as f:\n",
    "        x_train, y_train = f['x_train'], f['y_train']\n",
    "        x_test, y_test = f['x_test'], f['y_test']\n",
    "        return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Required Callback Function\n",
    "class printlearningrate(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        lr = K.eval(optimizer.lr)\n",
    "        Epoch_count = epoch + 1\n",
    "        print('\\n', \"Epoch:\", Epoch_count, ', LR: {:.2f}'.format(lr))\n",
    "\n",
    "printlr = printlearningrate() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJB9cyUAZ8b9"
   },
   "source": [
    "# Download MNIST Dataset\n",
    "\n",
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "As this is based off a kaggle turorial we'll download a `.npz` file from Kaggle. If you wish to see the process of downloading the unprocessed MNIST Dataset please refer to the [GANS notebook]()\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the file to download\n",
    "url = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Write to file\n",
    "with open(\"data_folder/mnist.npz\", \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "# Check file downloaded\n",
    "for filename in os.listdir(\"data_folder\"):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiUNHq7GbYum"
   },
   "source": [
    "# Building the Auto Encoder\n",
    "\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "Now we've downloaded the dataset 70,000 handwritten digits.\n",
    "    \n",
    "First we'll set up the model architecture: \n",
    "    \n",
    "* `ENOCIDING_DIM` is dimension of our latent space dimension - this value is trial and error\n",
    "* `encoded` and `decoded` are simply our encoded and decoded layers\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423,
     "referenced_widgets": [
      "685dd17bdbd242a6b8aae154168e11c1",
      "1884061b62a247c89ba5ec1f3d3bc87b",
      "7ed4f66c82614d58a2b51ca4284279ae",
      "49a2bbad317a42a8baafa935f3b1f24d",
      "8bf6c9121e0d492e9583973ceb432fe9",
      "852b3192156e40d7917b54706dce8984",
      "05c68d288d9b481b92de797ec708262f",
      "4b4a9796717a4f8488d8de07995908ab",
      "08e8491a188b4d4ba5508d93aecc4d55",
      "3d62e0765df84d449f831d47d1d43da0",
      "cbc8fd9400844d44bf43e1d39d536f4e",
      "bf0158687e584b83b757793403379a69",
      "d696e38e3080441788b535148f400c3e",
      "016a20ec7ebc4b448654310bdbc1ef04",
      "89a534457c424ba8bbc5297c991524a7",
      "db39e71f318643a78831a39ae9e9b223",
      "3593b4170d074f628eec5097b44fcf4e",
      "f1613593f4ca4f60be83b94cf9636e4a",
      "83c0ee38e3864a6583ee0b15b611f9d2",
      "8028397c30584cbd93961ec8819dc737",
      "0c360d889aa74a8b807f7ae0d29adbdb",
      "434dc2bb338544849897e59cef9c227b",
      "e3276a34cd4c4b26a3bce47551db7ffb",
      "9b6e1a6a950e43768decfdc896b14d95",
      "4304c3b333974f32a2fc4b317dde986c",
      "4f411ea3756f43aa98e2c2ac9e260402",
      "20d6f33ca8ec4a2da15c798ac303280d",
      "4930f5e49ae34727b2d565ba6abbba29",
      "b10e6d671c964b7b9998c03dafd69d1b",
      "4e05c783400c44dfbf1c918156aeface",
      "bcf3c439b54f45978300130c48a3a830",
      "2142f99e9f76444581bb3dfc46c8d8d6",
      "16373be927704a4586318a6bbf079b01",
      "6ed6edca08364822b93cccd9d51e59c9",
      "14b83f4fac694d60b4e34ea1187bc7e8",
      "faff639a61ce41c99bce8254a89a9e38",
      "63343150949148afa8687e3f454a4222",
      "7a01cc7bcdfb406092918c33b2d218f4",
      "62f2795555af46929d949dd5cde4e856",
      "51d83770272845efb84c480a94844112",
      "bbd12fed621b4a24bb09dc47a6061212",
      "6d6df8f8006e4229af72cda52b8f139c",
      "2e024e821363416594693388883e6a22",
      "b073f424f56f43ffa27e0673c33d8296"
     ]
    },
    "id": "T-vEr9Zaa6dz",
    "outputId": "8d87ef52-5a8a-423e-8a71-4a0dcbc1394e"
   },
   "outputs": [],
   "source": [
    "# we will start simple with a single fully-connected neural layer as encoder and decoder\n",
    "# this is the siez of our encoded representations\n",
    "ENCODING_DIM = 32\n",
    "\n",
    "# input placeholder\n",
    "input_img = tf.keras.layers.Input(shape=(784,))\n",
    "\n",
    "# this is the encoded representation of the input\n",
    "encoded = tf.keras.layers.Dense(ENCODING_DIM, activation='relu')(input_img)\n",
    "\n",
    "# this is the loss reconstruction of the input\n",
    "decoded = tf.keras.layers.Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its recommendation\n",
    "autoencoder = tf.keras.models.Model(input_img, decoded)\n",
    "\n",
    "# let's also create a seprate encoder model\n",
    "# this mode maps an input to its encoded representation\n",
    "encoder = tf.keras.models.Model(input_img, encoded)\n",
    "\n",
    "\n",
    "\n",
    "# as well as decoder model\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = tf.keras.layers.Input(shape=(ENCODING_DIM,))\n",
    "\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# create the decoder model\n",
    "decoder = tf.keras.models.Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9SpP12_fkzO"
   },
   "source": [
    "\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "\n",
    "As with other machine learning methods we can set optamizer and loss function. Here choosing the Adadelta optimizer and  per-pixel binary crossentropy loss\n",
    "\n",
    "    \n",
    "    \n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's train our autoencoder to reconstruct MNIST digits\n",
    "# first we will configure our model to use a per-pixel binary crossentropy loss, and the Adadelta optimizer\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# let's prepare our input data. We are using MNIST digits and we are disregrading the labels (since we are only interested in encoding/decoding the input images)\n",
    "# load the data\n",
    "(x_train, _), (x_test, _) = load_data('data_folder/mnist.npz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiFzz8XXbCpI"
   },
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "Let's confirm how many image samples we have, as well as get a sample of images to show. Here our training data is 6 times bigger than the test data\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33nD90rijMSo"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33nD90rijMSo"
   },
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "We're now ready to begin our training loop!     \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "Remember `if train_local:` was set early in the notebook after hardware checks etc\n",
    "\n",
    "`tf.keras.callbacks.LearningRateScheduler` allows us to change the learning rate in this case increasing with each Epoch. With `printlearningrate` you the learning rate and loss will be printed as the model is trained   .\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "you can alter the number of epochs by editing the line `E = 50` \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Epochs\n",
    "E = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_local:\n",
    "    # now let's train our autoencoder for 50 epochs\n",
    "    autoencoder.fit(x_train, x_train, epochs=E, batch_size=256, shuffle=True, \n",
    "                    validation_data=(x_test, x_test),\n",
    "                    callbacks=[ tf.keras.callbacks.LearningRateScheduler( lambda epoch: 1e-1 * 10 ** (epoch / E )),\n",
    "                                                                          printlr])\n",
    "    # Trained                 \n",
    "    trained = True\n",
    "else:\n",
    "    print('Train_local set to False loading pretrained')\n",
    "    try:\n",
    "        autoencoder.load_weights('Model_weights/AE1.ckpt')\n",
    "    except FileNotFoundError:\n",
    "         print('grabing pretrained data')\n",
    "\n",
    "if train_local:\n",
    "    if trained:\n",
    "        autoencoder.save_weights('AE1.ckpt')\n",
    "        print('save trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 50 epochs the autoencoder seems to reach a stable train/test loss value of about 0.11. \n",
    "# We can try to visualize the reconstructed inputs and the encoded representations. We will be using Matplotlib\n",
    "# encode and decode some digits\n",
    "# note that we take them from the \"test\" set\n",
    "\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# now using Matplotlib to plot the images\n",
    "n = 10 # how many images we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91acZT2CjvhX"
   },
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "# Sparse AutoEncoders\n",
    "    \n",
    "    \n",
    "Adding a sparsity constraint on the encoded representations The representations were only constrained by the size of the hidden layer (32). In such a situation, the hidden layer is learning an approximation of PCA. But another way to constain the representation to be compact is to add a sparsity constraint on the activity of the hiddne representations, so fewer units would \"fire\" at a given time. We can do this by `adding activity_regularizer` to our Dense layer.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODING_DIM = 32\n",
    "\n",
    "input_img = tf.keras.layers.Input(shape=(784,))\n",
    "\n",
    "# add a dense layer with L1 activity regularizer\n",
    "encoded = tf.keras.layers.Dense(ENCODING_DIM, activation='relu', \n",
    "                                activity_regularizer=tf.keras.regularizers.l1(10e-5))(input_img)\n",
    "decoded = tf.keras.layers.Dense(784, activation='sigmoid')(encoded)\n",
    "autoencoder = tf.keras.models.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "# let's also create a seprate encoder model\n",
    "# this mode maps an input to its encoded representation\n",
    "encoder = tf.keras.models.Model(input_img, encoded)\n",
    "\n",
    "\n",
    "\n",
    "# as well as decoder model\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = tf.keras.layers.Input(shape=(ENCODING_DIM,))\n",
    "\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# create the decoder model\n",
    "decoder = tf.keras.models.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "Let's train this for 100 epochs (with added regularization, the model is less likely to overfit and can be trained longer).\n",
    "\n",
    "you can alter the number of epochs by editing the line `E = 50` to `E = 100`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if train_local:\n",
    "    # The model ends with a train loss of 0.11 and test loss of 0.10. \n",
    "    # The difference is mostly due to the regularization term being added to the loss during training\n",
    "    autoencoder.fit(x_train, x_train, epochs=E, batch_size=256, shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-1 * 10 ** (epoch / E )),\n",
    "                                                                    printlr])\n",
    "   \n",
    "    # Trained                 \n",
    "    trained = True\n",
    "else:\n",
    "    print('Train_local set to False loading pretrained')\n",
    "    try:\n",
    "         autoencoder.load_weights('Model_weights/AE_sparse.ckpt')\n",
    "    except FileNotFoundError:\n",
    "         print('grabing pretrained data')\n",
    "\n",
    "if train_local:\n",
    "    if trained:\n",
    "        autoencoder.save_weights('Model_weights/AE_sparse.ckpt')\n",
    "\n",
    "\n",
    "        print('save trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "# now using Matplotlib to plot the images\n",
    "n = 10 # how many images we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u04qLbP9i_MP"
   },
   "source": [
    "\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    The images look pretty similar to the previous model, the only significant difference being the sparsity of the encoded representations. For 100 Epochs encoded_imgs.mean() yields a value of 3.33 over 10,000 test images. whereas with previous model the same quantity was 7.30. So our new model yields encoded representations that are twice sparser.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGj3vOkFfLnr"
   },
   "source": [
    "\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "# Deep autoencoders\n",
    "    \n",
    "We do not have to limit ourselves to a single layer as encoder or decoder, we could instead use a stack of layers, making our network more complex\n",
    "    \n",
    "Now we're going to decreae the dimensions down the layers for the encoder and increase the dimensions back up to the original dimension for the decoder. Other than adding in extra layers the model is built the same way (same optimizer, loss function etc)\n",
    "    \n",
    "Again Epochs is set with `E=50` and we can alter that if you wish\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_LYQv1qmFbQ"
   },
   "outputs": [],
   "source": [
    "input_img = tf.keras.layers.Input(shape=(784,))\n",
    "encoded = tf.keras.layers.Dense(128, activation='relu')(input_img)\n",
    "encoded = tf.keras.layers.Dense(64, activation='relu')(encoded)\n",
    "encoded = tf.keras.layers.Dense(32, activation='relu')(encoded)\n",
    "\n",
    "decoded = tf.keras.layers.Dense(64, activation='relu')(encoded)\n",
    "decoded = tf.keras.layers.Dense(128, activation='relu')(decoded)\n",
    "decoded = tf.keras.layers.Dense(784, activation='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try this\n",
    "autoencoder = tf.keras.models.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_local:\n",
    "    autoencoder.fit(x_train, x_train, epochs=E, batch_size=256, \n",
    "                shuffle=True, validation_data=(x_test, x_test),\n",
    "                callbacks=[ tf.keras.callbacks.LearningRateScheduler( lambda epoch: 1e-1 * 10 ** (epoch / E )),\n",
    "                           printlr])\n",
    "    # Trained                 \n",
    "    trained = True\n",
    "else:\n",
    "    print('Train_local set to False loading pretrained')\n",
    "    try:\n",
    "        autoencoder.load_weights('Model_weights/AE_deep.ckpt')\n",
    "    except FileNotFoundError:\n",
    "         print('grabing pretrained data')\n",
    "\n",
    "if train_local:\n",
    "    if trained:\n",
    "        autoencoder.save_weights('AE_deep.ckpt')\n",
    "        print('save trained')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HSP7csznZsY",
    "outputId": "25166c97-c6a1-4d8a-9461-91d422ba4241"
   },
   "outputs": [],
   "source": [
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "# now using Matplotlib to plot the images\n",
    "n = 10 # how many images we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuZXzuohnbwu"
   },
   "source": [
    "<hr>\n",
    "\n",
    "# Convolutional autoencoder\n",
    "\n",
    "\n",
    "<hr>\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "\n",
    "\n",
    "Since our inputs are images, it makes sense to use convolutional neural networks (CNNs) as encoders and decoders. In practical settings, autoencoders for images are always convolutional since they perform much better.\n",
    "\n",
    "This encoder will consist in a stack of `Conv2D` and `MaxPooling2D` layers (for spatial down-sampling), while the decoder will consist in a stack of `Conv2D` and `UpSampling2D` layers.\n",
    "\n",
    "</div>    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "T7AWo0ianfph",
    "outputId": "9aafa39e-45d7-43a0-b68a-d3cf0ee0f958"
   },
   "outputs": [],
   "source": [
    "input_img = tf.keras.layers.Input(shape=(28, 28, 1)) # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "x = tf.keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = tf.keras.models.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train this model we will with original MNIST digits with shape (samples, 3, 28, 28) \n",
    "# and we will just normalize pixel values between 0 and 1\n",
    "(x_train, _), (x_test, _) = load_data('data_folder/mnist.npz')\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "This will take a while so we'll use `TensorBoard` callback, this will allow the visulatiation of the training by visiting [http://0.0.0.0:6006](http://0.0.0.0:6006)  \n",
    "    \n",
    "Again you can alter the Epochs    \n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p tmp/autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_local:\n",
    "    autoencoder.fit(x_train, x_train, epochs=E, batch_size=128, shuffle=True, validation_data=(x_test, x_test),\n",
    "                callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./tmp/autoencoder'),\n",
    "                           tf.keras.callbacks.LearningRateScheduler( lambda epoch: 1e-1 * 10 ** (epoch / E )),\n",
    "                           printlr])\n",
    "    # Trained                 \n",
    "    trained = True\n",
    "else:\n",
    "    print('Train_local set to False loading pretrained')\n",
    "    try:\n",
    "        autoencoder.load_weights('Model_weights/AE_conv.ckpt')\n",
    "    except FileNotFoundError:\n",
    "         print('grabing pretrained data')\n",
    "\n",
    "if train_local:\n",
    "    if trained:\n",
    "        autoencoder.save_weights('Model_weights/AE_conv.ckpt')\n",
    "      \n",
    "\n",
    "        print('save trained')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKrM8J5EniQ7"
   },
   "source": [
    "\n",
    " \n",
    " \n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">    \n",
    "\n",
    "For 50 Epochs the model converges to a loss of 0.098, significantly better than previous model which is in large part due to the higher entropic capacity of the encoded representations (128 vs 32 previously).\n",
    "\n",
    "\n",
    "</div>    \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Denoising \n",
    "\n",
    "<hr>\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">  \n",
    "\n",
    "# Application to image denoising\n",
    "\n",
    "Let's put our convolutional autoencoder to work on an image denoising problem. We will train the autoencoder to map noisy digits images to clean digit images. We will generate synthetic noisy digits by applying a gaussian noise matrix and clip the images between 0 and 1.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = load_data('data_folder/mnist.npz')\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "Adding in synthetic noise, the plots below show the digits now hidden by noise    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor + np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
    "x_test_noisy = x_test + noise_factor + np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's what the noisy digits look like\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 2))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i + 1)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "\n",
    "    \n",
    "Can our autoencoder learn to recover the original digits?\n",
    "\n",
    "We will use a slightly different model with more filters per layer\n",
    "    \n",
    "\n",
    "</div>   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_img = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# the representation is (7, 7, 32)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = tf.keras.models.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "We'll train for 100 Epochs but you can return here and play around with different numbers of epochs\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if train_local:\n",
    "    # let's train for N epochs\n",
    "    autoencoder.fit(x_train_noisy, x_train, epochs=E, batch_size=128, shuffle=True, \n",
    "                validation_data=(x_test_noisy, x_test), \n",
    "                callbacks=[tf.keras.callbacks.TensorBoard(log_dir='./tmp/autoencoder', histogram_freq=0,\n",
    "                                                          write_graph=False),\n",
    "                           tf.keras.callbacks.LearningRateScheduler( lambda epoch: 1e-2 * 10 ** (epoch / E )),\n",
    "                           printlr])\n",
    "    # Trained                 \n",
    "    trained = True\n",
    "else:\n",
    "    print('Train_local set to False loading pretrained')\n",
    "    try:\n",
    "        autoencoder.load_weights('Model_weights/AE_noisy.ckpt')\n",
    "    except FileNotFoundError:\n",
    "         print('grabing pretrained data')\n",
    "\n",
    "if train_local:\n",
    "    if trained:\n",
    "        autoencoder.save_weights('Model_weights/AE_noisy.ckpt')\n",
    "       \n",
    "\n",
    "        print('save trained')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how it did\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "# Earth Science Examples\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "<h1> Data Infilling </h1>\n",
    " \n",
    "Here's an example from [Barth et al. 2020] where an autoencoder is used to reconstruct missing SST data.\n",
    "\n",
    "\n",
    "* SST observations from satellites can be obscured by cloud\n",
    "* An autoencoder was used to reconstruct the missing data based on the available cloud-free pixels in satellite images\n",
    "\n",
    "\n",
    "![SST fig Barth 2020](data_folder/figures/SST_Barth_clipped4.png)\n",
    "    \n",
    "Fig (a) shows satellite SST, (b) is with added cloud to allow for validation, c) The Autoencoder estimation of sea surface temperature, f) Estimation of SST using more traditional empirical orthogonal methods\n",
    "    \n",
    "* Neural networks can learn nonlinear, stochastic features measured at sea surface by satellite sensors. and their use might prove efficient in retraining small-scale structures that more traditional methods may remove / smooth out.    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "If you wish to look into this further, the original python code can be found on GitHub although the Authors have superceeded this with a Julia version:\n",
    "    \n",
    "* Code: [Git Hub Repo (Archived)](https://github.com/gher-uliege/DINCAE)\n",
    "* Julia Code: [GitHub Repo](https://github.com/gher-uliege/DINCAE.jl)\n",
    "* [Example Jupyter Notebook in Julia](https://github.com/gher-uliege/DINCAE.jl/blob/main/examples/DINCAE_tutorial.ipynb)\n",
    "    \n",
    "    \n",
    "**Please be aware the dataset used is very large 100GB+**\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Variational AutoEncoders\n",
    "\n",
    "<hr>\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "\n",
    "Variational autoencoder is where the decoder network can be used independently of the encoder to generate new states similar to the original inputs, by constraining the latent space to be both continuous and complete. Allowing  any point to be sampled and decoded back into something meaningful. This can be done for example in the form of a distribution: A normal distribution is parameterized by a mean (ùúá) and a variance (ùúé) and which is exactly (with some ‚Äúvariations‚Äù) what‚Äôs done in the case of a VAE.\n",
    "    \n",
    "![VAE](data_folder/figures/VAE.png)    \n",
    "[https://towardsdatascience.com/reparameterization-trick-126062cfd3c3](https://towardsdatascience.com/reparameterization-trick-126062cfd3c3)\n",
    "    \n",
    "An Earth Science application could be for ensemble based predictions and weather predictions or climate projections. As long as your distribution is representative of the possible space of inputs and it's potentially a much less computationally expensive way of generating a very large ensemble.\n",
    "\n",
    "It could be used as just a way of generating synthetic data and more generally, so a lot of us work with extreme data and often the issue with that for machine learning is there are not enough examples of extreme data points. So if you could use an autoencoder to just generate more examples of plausible extreme realizations and that could allow for doing further machine learning training with that kind of data.   \n",
    "    \n",
    "<h1> VAE example GCM emulator </h1>\n",
    "  \n",
    "Some work by Philip Brohan at the Met Office is detailed on Phillip Brohans website [https://brohan.org/ML_GCM/models/autoencoder/index.html](https://brohan.org/ML_GCM/models/autoencoder/index.html)  \n",
    "\n",
    "![VAE_Broahn](data_folder/figures/VAE_brohan.png)         \n",
    "    \n",
    "* Uses a VAE to emulate a General Circulation Model (GCM)\n",
    "* Trained on 2568T, T+6h pairs of weather states from reanalysis data\n",
    "* Used 4 surface variables: Temperature, Mean Sea Level Pressure, U wind, V wind at 2 degree resolution\n",
    "* Took 35 mins to train and runs 100,000 times faster than a conventional GCM\n",
    "* Provides plausible representations of the weather but with less variance.\n",
    "* Could be a great way of quickly estimating general trends    \n",
    "\n",
    "This is visualized on the site as a video of the estimated states with the latent space displayed in the corner.\n",
    "\n",
    "\n",
    "![Broahn](data_folder/figures/BrohanGCM.png)     \n",
    " \n",
    "\n",
    "The whole method along with the code can be found [here](https://brohan.org/ML_GCM/index.html)     \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "\n",
    "\n",
    "\n",
    "An encoder network turns the input samples x into two parameters in a latent space, which we will note as z_mean and z_log_sigma. Then we randomly sample similar points z from the latent normal distribution that is assumed to generate the data, via z = z_mean + exp(x_log_sigma) * epsilon, where epsilon is a random normal tensor. Finally, a deocded network maps these latent space points back to the original input data.\n",
    "\n",
    "The parameters of the model are trained via two loss functions: a reconstriction loss forcing the decoded samples to match the initial inputs (just like in our previous autoencoders), and the KL divergence between the learned latent distribution and the prior distribution, acting as a regulaization term. You could actually get rid of this latter term entirely, although it does help in learning well-formed latent spaces and reducing overfitting to the training data.\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # helper functions\n",
    "    \n",
    "def plot_results(models, data, batch_size=128, model_name=\"vae_mnist\"):\n",
    "    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n",
    "    # Arguments:\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "    # display a 30x30 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-4, 4, n)\n",
    "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">  \n",
    "once again we'll load in the data and set up our general model parameters\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_data('data_folder/mnist.npz')\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "original_dim = image_size * image_size\n",
    "x_train = np.reshape(x_train, [-1, original_dim])\n",
    "x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "E = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">  \n",
    "\n",
    "Here 2 dimensional latent space is made up of the mean and the variance.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = tf.keras.layers.Input(shape=input_shape, name='encoder_input')\n",
    "x = tf.keras.layers.Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = tf.keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = tf.keras.layers.Dense(latent_dim, name='z_log_var')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">  \n",
    "We can use these parameters to sample new similar points from the latent space:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.keras.backend.shape(z_mean)[0]\n",
    "    dim = tf.keras.backend.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.keras.backend.exp(0.5 * z_log_var) * epsilon\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use reparameterization trick to push the sampling out as input\n",
    "z = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = tf.keras.models.Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "# tf.keras.utils.plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = tf.keras.layers.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = tf.keras.layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = tf.keras.layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = tf.keras.models.Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "# tf.keras.utils.plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = tf.keras.models.Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">  \n",
    "    \n",
    "* an end-to-end autoencoder mapping inputs to reconstructions\n",
    "* an encoder mapping inputs to the latent space\n",
    "* a generator that can take points on the latent space and will output the corresponding reconstructed samples.\n",
    "\n",
    "We train the model using the end-to-end model, with a custom loss function: the sum of a reconstruction term, and the KL divergence regularization term.\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = (encoder, decoder)\n",
    "data = (x_test, y_test)\n",
    "# reconstruction_loss = tf.keras.losses.mse(inputs, outputs)\n",
    "reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, outputs)\n",
    "\n",
    "reconstruction_loss *= original_dim\n",
    "\n",
    "kl_loss = 1 + z_log_var - tf.keras.backend.square(z_mean) - tf.keras.backend.exp(z_log_var)\n",
    "kl_loss = tf.keras.backend.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = tf.keras.backend.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()\n",
    "# tf.keras.utils.plot_model(vae, to_file='vae_mlp.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_local:\n",
    "    # let's train for N epochs\n",
    "    vae.fit(x_train, epochs=E, batch_size=batch_size, validation_data=(x_test, None))\n",
    "    # Trained                 \n",
    "    trained = True\n",
    "else:\n",
    "    print('Train_local set to False loading pretrained')\n",
    "    try:\n",
    "        vae.load_weights('Model_weights/vae_mlp.mnist.h5')\n",
    "    except FileNotFoundError:\n",
    "         print('grabing pretrained data')\n",
    "\n",
    "if train_local:\n",
    "    if trained:\n",
    "        vae.save_weights('vae_mlp.mnist.h5')\n",
    "\n",
    "\n",
    "        print('save trained')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Visualizing latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Because the VAE is a generative model, we can also use it to generate new digits! \n",
    "#Here we will scan the latent plane, sampling latent points at regular intervals and \n",
    "# generating the corresponding digit for each of these points. \n",
    "# This gives us a visualization of the latent manifold that \"generates\" the MNIST digits.\n",
    "plot_results(models, data, batch_size=batch_size, model_name='vae_mlp')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">  \n",
    "\n",
    "Finally above is an example of how you can kind of plot the latent space. These are just the the two dimensions of the latent space and all these points where the input date has mapped to, coloured based on what the original digit was. \n",
    "    \n",
    "So the lower numbers at the blue end, to higher numbers at the red end. You can clearly see here the clustering so that each of the the digits in certain areas of of the latent space. Something in this latent representation is capturing kind of common features for specific Number classifications so that will then be useful when it's decoded back up for the the network to predict generally.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of gan_intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "016a20ec7ebc4b448654310bdbc1ef04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83c0ee38e3864a6583ee0b15b611f9d2",
      "max": 28881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8028397c30584cbd93961ec8819dc737",
      "value": 28881
     }
    },
    "05c68d288d9b481b92de797ec708262f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "08e8491a188b4d4ba5508d93aecc4d55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0c360d889aa74a8b807f7ae0d29adbdb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14b83f4fac694d60b4e34ea1187bc7e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62f2795555af46929d949dd5cde4e856",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_51d83770272845efb84c480a94844112",
      "value": ""
     }
    },
    "16373be927704a4586318a6bbf079b01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1884061b62a247c89ba5ec1f3d3bc87b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_852b3192156e40d7917b54706dce8984",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_05c68d288d9b481b92de797ec708262f",
      "value": ""
     }
    },
    "20d6f33ca8ec4a2da15c798ac303280d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2142f99e9f76444581bb3dfc46c8d8d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e024e821363416594693388883e6a22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3593b4170d074f628eec5097b44fcf4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d62e0765df84d449f831d47d1d43da0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4304c3b333974f32a2fc4b317dde986c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e05c783400c44dfbf1c918156aeface",
      "max": 1648877,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bcf3c439b54f45978300130c48a3a830",
      "value": 1648877
     }
    },
    "434dc2bb338544849897e59cef9c227b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4930f5e49ae34727b2d565ba6abbba29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "49a2bbad317a42a8baafa935f3b1f24d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d62e0765df84d449f831d47d1d43da0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cbc8fd9400844d44bf43e1d39d536f4e",
      "value": " 9913344/? [00:00&lt;00:00, 20994936.87it/s]"
     }
    },
    "4b4a9796717a4f8488d8de07995908ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e05c783400c44dfbf1c918156aeface": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f411ea3756f43aa98e2c2ac9e260402": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2142f99e9f76444581bb3dfc46c8d8d6",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_16373be927704a4586318a6bbf079b01",
      "value": " 1649664/? [00:00&lt;00:00, 3713281.49it/s]"
     }
    },
    "51d83770272845efb84c480a94844112": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62f2795555af46929d949dd5cde4e856": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63343150949148afa8687e3f454a4222": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e024e821363416594693388883e6a22",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b073f424f56f43ffa27e0673c33d8296",
      "value": " 5120/? [00:00&lt;00:00, 9856.68it/s]"
     }
    },
    "685dd17bdbd242a6b8aae154168e11c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1884061b62a247c89ba5ec1f3d3bc87b",
       "IPY_MODEL_7ed4f66c82614d58a2b51ca4284279ae",
       "IPY_MODEL_49a2bbad317a42a8baafa935f3b1f24d"
      ],
      "layout": "IPY_MODEL_8bf6c9121e0d492e9583973ceb432fe9"
     }
    },
    "6d6df8f8006e4229af72cda52b8f139c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6ed6edca08364822b93cccd9d51e59c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_14b83f4fac694d60b4e34ea1187bc7e8",
       "IPY_MODEL_faff639a61ce41c99bce8254a89a9e38",
       "IPY_MODEL_63343150949148afa8687e3f454a4222"
      ],
      "layout": "IPY_MODEL_7a01cc7bcdfb406092918c33b2d218f4"
     }
    },
    "7a01cc7bcdfb406092918c33b2d218f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ed4f66c82614d58a2b51ca4284279ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b4a9796717a4f8488d8de07995908ab",
      "max": 9912422,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08e8491a188b4d4ba5508d93aecc4d55",
      "value": 9912422
     }
    },
    "8028397c30584cbd93961ec8819dc737": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83c0ee38e3864a6583ee0b15b611f9d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "852b3192156e40d7917b54706dce8984": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89a534457c424ba8bbc5297c991524a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c360d889aa74a8b807f7ae0d29adbdb",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_434dc2bb338544849897e59cef9c227b",
      "value": " 29696/? [00:00&lt;00:00, 9790.73it/s]"
     }
    },
    "8bf6c9121e0d492e9583973ceb432fe9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b6e1a6a950e43768decfdc896b14d95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4930f5e49ae34727b2d565ba6abbba29",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b10e6d671c964b7b9998c03dafd69d1b",
      "value": ""
     }
    },
    "b073f424f56f43ffa27e0673c33d8296": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b10e6d671c964b7b9998c03dafd69d1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbd12fed621b4a24bb09dc47a6061212": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcf3c439b54f45978300130c48a3a830": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bf0158687e584b83b757793403379a69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d696e38e3080441788b535148f400c3e",
       "IPY_MODEL_016a20ec7ebc4b448654310bdbc1ef04",
       "IPY_MODEL_89a534457c424ba8bbc5297c991524a7"
      ],
      "layout": "IPY_MODEL_db39e71f318643a78831a39ae9e9b223"
     }
    },
    "cbc8fd9400844d44bf43e1d39d536f4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d696e38e3080441788b535148f400c3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3593b4170d074f628eec5097b44fcf4e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f1613593f4ca4f60be83b94cf9636e4a",
      "value": ""
     }
    },
    "db39e71f318643a78831a39ae9e9b223": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e3276a34cd4c4b26a3bce47551db7ffb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9b6e1a6a950e43768decfdc896b14d95",
       "IPY_MODEL_4304c3b333974f32a2fc4b317dde986c",
       "IPY_MODEL_4f411ea3756f43aa98e2c2ac9e260402"
      ],
      "layout": "IPY_MODEL_20d6f33ca8ec4a2da15c798ac303280d"
     }
    },
    "f1613593f4ca4f60be83b94cf9636e4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "faff639a61ce41c99bce8254a89a9e38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbd12fed621b4a24bb09dc47a6061212",
      "max": 4542,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6d6df8f8006e4229af72cda52b8f139c",
      "value": 4542
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
